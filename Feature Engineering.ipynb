{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Mongodb path to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"myApp\") \\\n",
    "   .config(\"spark.mongodb.input.uri\", \"mongodb://18.221.66.227/project.small_data\")\\\n",
    "   .config(\"spark.mongodb.output.uri\", \"mongodb://18.221.66.227/project.test\")\\\n",
    "   .getOrCreate()\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- acoustic_data: integer (nullable = true)\n",
      " |-- time_to_failure: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('small_train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(x, lines=10000):\n",
    "    f = list()\n",
    "    for i in range(lines):\n",
    "        row = np.random.randint(len(x)- 150000)\n",
    "        data = filter(x, row)\n",
    "        feature = dict()\n",
    "        x_val = np.array([i[0] for i in data])\n",
    "        feature['y_target'] = float(data[-1,1])\n",
    "        feature['mean'] = float(x_val.mean())\n",
    "        feature['std'] = float(x_val.std())\n",
    "        feature['max1'] = float(x_val.max())\n",
    "        feature['min1'] = float(x_val.min())\n",
    "        feature['sum1'] = float(x_val.sum())\n",
    "        feature['abs_max'] = float(np.abs(x_val).sum())\n",
    "        feature['mean_change_abs'] = float(np.mean(np.diff(x_val)))\n",
    "        feature['mean_change_rate'] = float(np.mean(np.nonzero((np.diff(x_val) / x_val[:-1]))[0]))\n",
    "        feature['abs_max'] = float(np.abs(x_val).max())\n",
    "        feature['abs_min'] = float(np.abs(x_val).min())\n",
    "        feature['std_first_50000'] = float(x_val[:50000].std())\n",
    "        feature['std_last_50000'] = float(x_val[-50000:].std())\n",
    "        feature['std_first_10000'] = float(x_val[:10000].std())\n",
    "        feature['std_last_10000'] = float(x_val[-10000:].std())\n",
    "        feature['avg_first_50000'] = float(x_val[:50000].mean())\n",
    "        feature['avg_last_50000'] = float(x_val[-50000:].mean())\n",
    "        feature['avg_first_10000'] = float(x_val[:10000].mean())\n",
    "        feature['avg_last_10000'] = float(x_val[-10000:].mean())\n",
    "        feature['min_first_50000'] = float(x_val[:50000].min())\n",
    "        feature['min_last_50000'] = float(x_val[-50000:].min())\n",
    "        feature['min_first_10000'] = float(x_val[:10000].min())\n",
    "        feature['min_last_10000'] = float(x_val[-10000:].min())\n",
    "        feature['max_first_50000'] = float(x_val[:50000].max())\n",
    "        feature['max_last_50000'] = float(x_val[-50000:].max())\n",
    "        feature['max_first_10000'] = float(x_val[:10000].max())\n",
    "        feature['max_last_10000'] = float(x_val[-10000:].max())\n",
    "        feature['max_to_min'] = float(x_val.max() / np.abs(x_val.min()))\n",
    "        feature['max_to_min_diff'] = float(x_val.max() - np.abs(x_val.min()))\n",
    "        feature['count_big'] = float(len(x_val[np.abs(x_val) > 500]))\n",
    "        feature['mean_change_rate_first_50000'] = float(np.mean(np.nonzero((np.diff(x_val[:50000]) / x_val[:50000][:-1]))[0]))\n",
    "        feature['mean_change_rate_last_50000'] = float(np.mean(np.nonzero((np.diff(x_val[-50000:]) / x_val[-50000:][:-1]))[0]))\n",
    "        feature['mean_change_rate_first_10000'] = float(np.mean(np.nonzero((np.diff(x_val[:10000]) / x_val[:10000][:-1]))[0]))\n",
    "        feature['mean_change_rate_last_10000'] = float(np.mean(np.nonzero((np.diff(x_val[-10000:]) / x_val[-10000:][:-1]))[0]))\n",
    "\n",
    "        feature['q70'] = float(np.quantile(x_val, 0.70)) \n",
    "        feature['q75'] = float(np.quantile(x_val, 0.75)) \n",
    "        feature['q60'] = float(np.quantile(x_val, 0.60))\n",
    "        feature['q65'] = float(np.quantile(x_val, 0.65)) \n",
    "        feature['q85'] = float(np.quantile(x_val, 0.85))\n",
    "        feature['q90'] = float(np.quantile(x_val, 0.90))\n",
    "        feature['q80'] = float(np.quantile(x_val, 0.80))\n",
    "        feature['q95'] = float(np.quantile(x_val, 0.95))\n",
    "        feature['q99'] = float(np.quantile(x_val, 0.99))\n",
    "        f.append(feature)\n",
    "    return f\n",
    "\n",
    "def filter(x, row_num):\n",
    "    df_size = len(x)\n",
    "    if (row_num > (df_size-150000)):\n",
    "        return \"Can't be computed it.\"\n",
    "    else:\n",
    "        return x[row_num: row_num+150000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to calculate features and push 10K data points in a go to Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    data = np.array(pd.read_csv('small_train.csv'))\n",
    "    f = calculate(data)\n",
    "    del data\n",
    "    data = spark.sparkContext.parallelize(f).toDF()\n",
    "    data.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each batch of 10K data takes 13 min to process on local system**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
